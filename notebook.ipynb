{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4mGf7OfZG2I"
      },
      "source": [
        "#Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numpy==1.23.1"
      ],
      "metadata": {
        "id": "8fAv4YKdTqI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plQCbAEqdDT0",
        "outputId": "2698ae4e-b696-46e7-b5a0-888034afbfee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import os\n",
        "import h5py\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn4Ct4PkZNah"
      },
      "source": [
        "#Define paths and load the data, setup the label binarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvusVELTdZkF"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/GMM Group'\n",
        "model_path = os.path.join(path, 'model0516_3noaug.h5')\n",
        "hdf5_file = h5py.File(os.path.join(path, 'EPIC_audio.hdf5'), 'r')\n",
        "label_binarizer = MultiLabelBinarizer(classes=np.arange(44))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--9ysiD4ZVFD"
      },
      "source": [
        "#Define a generator function (base_generator) to yield audio segments and their labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpyBN4JgT6ff"
      },
      "outputs": [],
      "source": [
        "def base_generator(annotation_file, hdf5_file, segment_length=3, step=1, sample_rate=24000, augment=False):\n",
        "  annotations = pd.read_csv(os.path.join(path, annotation_file))\n",
        "  segment_length_with_sample_rate = segment_length * sample_rate\n",
        "  for key in annotations['video_id'].unique():\n",
        "    audio_data = hdf5_file[key]\n",
        "    file_annotations = annotations[annotations['video_id'] == key]\n",
        "    frames = tf.signal.frame(audio_data, segment_length*sample_rate, sample_rate, axis=0, pad_end=True)\n",
        "    for index, frame in enumerate(frames):\n",
        "      start_segment = index * step * sample_rate\n",
        "      stop_segment = start_segment + segment_length * sample_rate\n",
        "      \n",
        "      # if class is shorter than one second:\n",
        "      full_sample = ((file_annotations[\"stop_sample\"] - file_annotations[\"start_sample\"]) < 24000) & \\\n",
        "              (file_annotations[\"start_sample\"] >= start_segment) & \\\n",
        "              (file_annotations[\"stop_sample\"] <= stop_segment)\n",
        "\n",
        "      # if class is longer than one second:\n",
        "      part_sample = ((file_annotations[\"stop_sample\"] - file_annotations[\"start_sample\"]) >= 24000) & \\\n",
        "              ((stop_segment - file_annotations['start_sample'] > 24000) & (file_annotations['stop_sample'] - start_segment > 24000))\n",
        "\n",
        "      frame = frame.numpy()\n",
        "\n",
        "      if augment:\n",
        "        choice = random.choice([1,2,3])\n",
        "        if choice == 1:\n",
        "          frame = librosa.effects.pitch_shift(frame, sr=sample_rate, n_steps=4)\n",
        "        elif choice == 2:\n",
        "          noise_factor = 0.005\n",
        "          white_noise = np.random.randn(len(frame)) * noise_factor\n",
        "          frame = frame + white_noise\n",
        "          \n",
        "      filtered_samples = file_annotations[full_sample | part_sample]\n",
        "      class_ids = np.expand_dims(filtered_samples['class_id'].to_numpy(), axis = 0)\n",
        "      label_vector = label_binarizer.fit_transform(class_ids).reshape((44))\n",
        "      \n",
        "      \n",
        "      spectrogram = librosa.feature.melspectrogram(y=frame, sr=sample_rate, n_mels=128)\n",
        "      log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
        "      log_spectrogram = np.expand_dims(log_spectrogram, axis=2)\n",
        "\n",
        "      yield (log_spectrogram, label_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Nc3hQKZa21"
      },
      "source": [
        "#Define train, validation generators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1Jq_Zj3T8sc"
      },
      "outputs": [],
      "source": [
        "def train_generator():\n",
        "  return base_generator('EPIC_Sounds_train_full.csv', hdf5_file, augment=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty-wu3HLT9o5"
      },
      "outputs": [],
      "source": [
        "def validation_generator():\n",
        "  return base_generator('EPIC_Sounds_validation_full.csv', hdf5_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOpE_QaeZfLV"
      },
      "source": [
        "#Create train, validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFTaoHOTNbdl"
      },
      "outputs": [],
      "source": [
        "# Define output shapes and types for your data and labels\n",
        "output_shapes = ((128, 141, 1), (44),)\n",
        "output_types = (tf.float32, tf.float32)\n",
        "\n",
        "# Create dataset from generator\n",
        "train_dataset = tf.data.Dataset.from_generator(train_generator, output_shapes=output_shapes, output_types=output_types)\n",
        "val_dataset = tf.data.Dataset.from_generator(validation_generator, output_shapes=output_shapes, output_types=output_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWt9OB49OWyx"
      },
      "outputs": [],
      "source": [
        "# Optional: Set batch size and enable prefetch for performance\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6yQJFG_oVHH"
      },
      "outputs": [],
      "source": [
        "val_dataset = val_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.cache()\n",
        "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrdjvNoxZkC0"
      },
      "source": [
        "#Build a model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iURqXMYPLc2V",
        "outputId": "73dfd8d6-9280-41c3-ec5a-26926811a5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 128, 141, 32)      832       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 128, 141, 32)     128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 128, 141, 32)      0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 128, 141, 32)      9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 128, 141, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 128, 141, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 64, 70, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 70, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 64, 70, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 64, 70, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 64, 70, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 64, 70, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 64, 70, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 32, 35, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 35, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 32, 35, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 32, 35, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 16, 18, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 16, 18, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 16, 18, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 16, 18, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 8, 9, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 9, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 8, 9, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 8, 9, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 4, 5, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 4, 5, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 4, 5, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 4, 5, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 2, 3, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1536)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              1573888   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               131200    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 44)                5676      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,886,796\n",
            "Trainable params: 2,884,876\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    # input shape (128,141,1)\n",
        "    # 1st set of layers: Conv2d+BatchNormalization+Relu --> 128,141,1 becomes 128, 141, 32\n",
        "    keras.layers.Conv2D(32,(5,5),padding=\"same\",input_shape=(128,141,1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('relu'),\n",
        "    # 2nd set of layers: Conv2d+BatchNormalization+Relu+MaxPooling2D --> 128, 141, 32 becomes 64, 70, 32\n",
        "    keras.layers.Conv2D(32,(3,3),padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.MaxPooling2D(),\n",
        "    # 3rd set of layers: Conv2d+BatchNormalization+Relu --> 64, 70, 32 becomes 64, 70, 64\n",
        "    keras.layers.Conv2D(64,(3,3),padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('relu'),\n",
        "    # 4th set of layers: Conv2d+BatchNormalization+Relu+MaxPooling2D --> 64, 70, 64 becomes 32, 35, 64\n",
        "    keras.layers.Conv2D(64,(3,3),padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.MaxPooling2D(padding=\"same\"),\n",
        "    # 5th set of layers: Conv2d+BatchNormalization+Relu+MaxPooling2D --> 32, 35, 64 becomes 16, 18, 128\n",
        "    keras.layers.Conv2D(128,(3,3),padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.MaxPooling2D(padding=\"same\"),\n",
        "    # 5th set of layers: Conv2d+BatchNormalization+Relu+MaxPooling2D --> 16, 18, 128 becomes 8, 9, 128\n",
        "    keras.layers.Conv2D(128,(3,3),padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.MaxPooling2D(padding=\"same\"),\n",
        "    # 5th set of layers: Conv2d+BatchNormalization+Relu+MaxPooling2D --> 8, 9, 128 becomes 4, 5, 256\n",
        "    keras.layers.Conv2D(256,(3,3),padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.MaxPooling2D(padding=\"same\"),\n",
        "    # 5th set of layers: Conv2d+BatchNormalization+Relu+MaxPooling2D --> 4, 5, 256 becomes 2, 3, 256\n",
        "    keras.layers.Conv2D(256,(3,3),padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation('relu'),\n",
        "    keras.layers.MaxPooling2D(padding=\"same\"),\n",
        "    # flatten the image --> 2, 3, 256 becomes 1536\n",
        "    keras.layers.Flatten(),\n",
        "    # go through a bunch of neurons and drop some of the links --> 1536 becomes 1024\n",
        "    keras.layers.Dense(1024, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    # go through another bunch of neurons and drop some of the links --> 1024 becomes 128\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    # finally go through 44 neurons --> 128 becomes 44\n",
        "    keras.layers.Dense(44, activation=\"sigmoid\")\n",
        "])\n",
        "# compile model for categorical results\n",
        "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7cMZZ_zZn06"
      },
      "source": [
        "#Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7jcPTCpLlOq"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n",
        "]\n",
        "\n",
        "history = model.fit(train_dataset, \n",
        "                    validation_data=val_dataset,\n",
        "                    epochs=50,\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTJVqU8MZv05"
      },
      "source": [
        "If needed load or save the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar-VapC9usu3"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(model_path)\n",
        "# model.save(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yh2Hsf4ZpXO"
      },
      "source": [
        "#Create test generator, test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYvG4ut9lp87"
      },
      "outputs": [],
      "source": [
        "def test_generator():\n",
        "  return base_generator('EPIC_Sounds_test_full.csv', hdf5_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhwxGY0yl2Zr"
      },
      "outputs": [],
      "source": [
        "test_dataset = tf.data.Dataset.from_generator(test_generator, output_shapes=output_shapes, output_types=output_types)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.cache()\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTpr9sWaWm7K"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukh1w33VO63U"
      },
      "source": [
        "740/740 [==============================] - 342s 462ms/step - loss: 0.0849 - accuracy: 0.3668\n",
        "\n",
        "[0.0848626121878624, 0.3667624294757843]\n",
        "\n",
        "No augmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSjg-syJO6cs"
      },
      "source": [
        "740/740 [==============================] - 347s 467ms/step - loss: 0.0767 - accuracy: 0.3772\n",
        "\n",
        "[0.07674212008714676, 0.3771544396877289]\n",
        "\n",
        "With augmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbTsiGuNbk3T"
      },
      "source": [
        "#Calculate metrics, predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "FpLi5criS2Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOHpN9HGQH_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3460b66-4437-4904-e5e5-b51d1368d95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "740/740 [==============================] - 75s 101ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLBNQT6jQhwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671121c9-989d-4819-95cc-a5199f089565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-4bf6cf54fa73>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  y_true_labels = np.array([np.where(row==1)[0] for row in y_true])\n"
          ]
        }
      ],
      "source": [
        "y_true = []\n",
        "for _, label in test_dataset:\n",
        "    y_true.extend(label.numpy().tolist())\n",
        "y_true = np.array(y_true)\n",
        "y_true_labels = np.array([np.where(row==1)[0] for row in y_true])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4axpnLrT5oX",
        "outputId": "bc2c5f46-1ed3-4b92-d5ce-2e65970029e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5143629604596147"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "top1_predictions = np.argmax(predictions, axis=1)\n",
        "top1_correct = [pred in true for pred, true in zip(top1_predictions, y_true_labels)]\n",
        "np.mean(top1_correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQA9s2pUXe0u",
        "outputId": "bca076eb-1ae8-46d3-ddc4-e8af6802c349"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.805888813788442"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "top5_predictions = np.argsort(predictions, axis=1)[:, -5:]\n",
        "top5_correct = [any(pred in true for pred in top5_pred) for top5_pred, true in zip(top5_predictions, y_true_labels)]\n",
        "np.mean(top5_correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI0iB_QfOy_R"
      },
      "outputs": [],
      "source": [
        "predictions_binary = np.where(predictions > 0.5, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m992PWNKQFik",
        "outputId": "bdbb09e3-a3de-4860-b050-67506e070d73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2892981491166879"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score = f1_score(y_true, predictions_binary, average='weighted')\n",
        "f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ_Bh-L4MqLM"
      },
      "source": [
        "#Model with augmentations:\n",
        "Top 1: 0.5306691449814126\n",
        "\n",
        "Top 5: 0.8186887461980399\n",
        "\n",
        "F1: 0.32247597425305213"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq1Uzu3UM9z-"
      },
      "source": [
        "#Model without augmentations:\n",
        "Top 1: 0.5143207164582629\n",
        "\n",
        "Top 5: 0.8060155457924975\n",
        "\n",
        "F1: 0.2892981491166879"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k40O9SB4Z3oR"
      },
      "source": [
        "#Functions to 'warm-up' the gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPs-YIs3S-tR",
        "outputId": "1ce81031-5521-4656-bb98-812f36b31f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8v241htS_wx",
        "outputId": "77618038-4863-4263-8005-6ced253b9daa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=3984.2676>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}